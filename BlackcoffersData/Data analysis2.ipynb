{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb7ab50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp39-cp39-win_amd64.whl (483 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp39-cp39-win_amd64.whl (122 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.6.3-py3-none-any.whl (395 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.10-cp39-cp39-win_amd64.whl (25 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.8-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Collecting thinc<8.3.0,>=8.2.2\n",
      "  Downloading thinc-8.2.3-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Collecting pydantic-core==2.16.3\n",
      "  Downloading pydantic_core-2.16.3-cp39-none-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.11-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.4)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: pydantic-core, colorama, catalogue, annotated-types, srsly, pydantic, murmurhash, cymem, wasabi, typer, smart-open, preshed, confection, cloudpathlib, blis, weasel, thinc, spacy-loggers, spacy-legacy, langcodes, spacy\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 colorama-0.4.6 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.6.3 pydantic-core-2.16.3 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e0b1413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "Collecting nltk>=3.8\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2021.8.3)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Installing collected packages: nltk, textblob\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.6.5\n",
      "    Uninstalling nltk-3.6.5:\n",
      "      Successfully uninstalled nltk-3.6.5\n",
      "Successfully installed nltk-3.8.1 textblob-0.18.0.post0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8f0408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "Collecting pyphen\n",
      "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.14.0 textstat-0.7.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000183C31FB340>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/textstat/\n"
     ]
    }
   ],
   "source": [
    "pip install textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a8fe4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>Article_Title</th>\n",
       "      <th>Article_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>Rising IT cities and its impact on the economy...</td>\n",
       "      <td>Grafana Dashboard to visualize and analyze sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>Rising IT Cities and Their Impact on the Econo...</td>\n",
       "      <td>Grafana Dashboard to visualize and analyze sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>Internet Demand's Evolution, Communication Imp...</td>\n",
       "      <td>Grafana Dashboard to visualize and analyze sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>Rise of Cybercrime and its Effect in upcoming ...</td>\n",
       "      <td>Grafana Dashboard to visualize and analyze sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>OTT platform and its impact on the entertainme...</td>\n",
       "      <td>Grafana Dashboard to visualize and analyze sen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                      Article_Title  \\\n",
       "0  blackassign0001  Rising IT cities and its impact on the economy...   \n",
       "1  blackassign0002  Rising IT Cities and Their Impact on the Econo...   \n",
       "2  blackassign0003  Internet Demand's Evolution, Communication Imp...   \n",
       "3  blackassign0004  Rise of Cybercrime and its Effect in upcoming ...   \n",
       "4  blackassign0005  OTT platform and its impact on the entertainme...   \n",
       "\n",
       "                                        Article_Text  \n",
       "0  Grafana Dashboard to visualize and analyze sen...  \n",
       "1  Grafana Dashboard to visualize and analyze sen...  \n",
       "2  Grafana Dashboard to visualize and analyze sen...  \n",
       "3  Grafana Dashboard to visualize and analyze sen...  \n",
       "4  Grafana Dashboard to visualize and analyze sen...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the extracted data from extracted_data.xlsx\n",
    "extracted_data_path = r'C:\\Users\\User\\Downloads\\extracted_data.xlsx'\n",
    "extracted_df = pd.read_excel(extracted_data_path)\n",
    "\n",
    "extracted_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de3cb04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9fe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a28be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a08e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.data.path.append('/C:/Users/User/Downloads/StopWords/')\n",
    "\n",
    "extracted_data_path = r'C:\\Users\\User\\Downloads\\extracted_data.xlsx'\n",
    "extracted_df = pd.read_excel(extracted_data_path)\n",
    "\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the extracted data from extracted_data.xlsx\n",
    "extracted_data_path = r'C:\\Users\\User\\Downloads\\extracted_data.xlsx'\n",
    "extracted_df = pd.read_excel(extracted_data_path)\n",
    "\n",
    "def compute_variables(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Compute variables\n",
    "    word_count = len(tokens)\n",
    "    filtered_word_count = len(filtered_tokens)\n",
    "    avg_sentence_length = word_count / text.count('.') \n",
    "    percentage_complex_words = (filtered_word_count / word_count) * 100\n",
    "    \n",
    "    # Perform sentiment analysis\n",
    "    blob = TextBlob(text)\n",
    "    polarity_score = blob.sentiment.polarity\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "    \n",
    "    return {\n",
    "        'Word_Count': word_count,\n",
    "        'Filtered_Word_Count': filtered_word_count,\n",
    "        'Avg_Sentence_Length': avg_sentence_length,\n",
    "        'Percentage_Complex_Words': percentage_complex_words,\n",
    "        'Polarity_Score': polarity_score,\n",
    "        'Subjectivity_Score': subjectivity_score\n",
    "    }\n",
    "\n",
    "# Compute variables\n",
    "variable_data = []\n",
    "for index, row in extracted_df.iterrows():\n",
    "    article_text = row['Article_Text']\n",
    "    variables = compute_variables(article_text)\n",
    "    variable_data.append(variables)\n",
    "\n",
    "variable_df = pd.DataFrame(variable_data)\n",
    "\n",
    "output_df = pd.concat([extracted_df, variable_df], axis=1)\n",
    "\n",
    "# Save the output DataFrame\n",
    "output_file_path = r'C:\\Users\\User\\Downloads\\output_data_structure.xlsx'\n",
    "output_df.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85474ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
